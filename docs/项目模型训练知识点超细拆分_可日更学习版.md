# MiniMind 模型训练知识点超细拆分（可直接做每日学习+自媒体输出）

> 适用人群：AI 零基础、想边学边做内容、后续做课程和创业的人。  
> 目标：把“模型训练”从一个大词拆成可执行的、低心理负担的最小任务单元。  
> 用法：每天只做 1~3 个小点，做完就能产出 1 条自媒体素材（图文/短视频/笔记）。

---

## 0. 先把全项目训练链路讲清楚（你每天在学哪一段）

MiniMind 的训练不是一个脚本，而是一条“能力流水线”：

1. **Tokenizer（可选）**：决定模型“看到文字时如何切分 token”。
2. **Pretrain（预训练）**：学习“世界知识”和“语言规律”。
3. **SFT（监督微调）**：学习“聊天格式、指令跟随”。
4. **LoRA（参数高效微调）**：在小显卡上低成本继续微调。
5. **DPO（偏好优化）**：让输出更符合“人类偏好”。
6. **Reason（推理蒸馏）**：强化 `<think>/<answer>` 风格和推理结构。
7. **Distillation（白盒蒸馏）**：小模型学习大模型的概率分布。
8. **PPO/GRPO/SPO（强化学习）**：用奖励模型持续优化回答质量。
9. **Eval/部署**：验证模型是否真的变好了。

对应脚本一一可追踪：

- `trainer/train_tokenizer.py`
- `trainer/train_pretrain.py`
- `trainer/train_full_sft.py`
- `trainer/train_lora.py`
- `trainer/train_dpo.py`
- `trainer/train_reason.py`
- `trainer/train_distillation.py`
- `trainer/train_ppo.py` / `trainer/train_grpo.py` / `trainer/train_spo.py`
- `eval_llm.py`

---

## 1. 训练前必须吃透的“总论知识点”（建议先学 7 天）

### 1.1 模型训练最小闭环（必背）

- 数据 -> 分词 -> `input_ids`
- 模型前向 -> `logits`
- 损失函数 -> `loss`
- 反向传播 -> `backward()`
- 优化器更新 -> `optimizer.step()`
- 清梯度 -> `optimizer.zero_grad()`

**自媒体选题**：
- 《我终于看懂了：大模型训练其实就 6 步》
- 《loss 到底是什么，为什么它一直降？》

### 1.2 你一定会反复遇到的训练组件

- **Batch size**：一次喂多少样本。
- **Accumulation steps**：小显存时“假装大 batch”。
- **Learning rate**：每次参数移动的步长。
- **Grad clip**：防止梯度爆炸。
- **Mixed precision (`bfloat16/float16`)**：省显存提速度。
- **Checkpoint**：断点续训与权重保存。
- **DDP**：多卡并行训练。

**自媒体选题**：
- 《为什么 8G 显卡也能训？核心是梯度累积》
- 《学习率不是越大越快，太大直接炸》

---

## 2. 数据与样本构造知识点（dataset 层）

核心文件：`dataset/lm_dataset.py`

### 2.1 PretrainDataset（预训练数据）

- 输入字段：`text`
- 处理逻辑：文本 -> token -> 加 BOS/EOS -> pad 到固定长度
- 标签逻辑：`labels = input_ids`，pad 部分置 `-100`（忽略损失）

**必须理解的小点**：
- 为什么要 `-100`？（CrossEntropy ignore_index）
- 为什么要固定长度？（批处理效率）
- 为什么要 BOS/EOS？（边界感知）

### 2.2 SFTDataset（指令微调数据）

- 输入字段：`conversations`
- `apply_chat_template` 生成聊天串
- 只对 assistant 回答部分打标签，用户问题部分通常是 `-100`

**必须理解的小点**：
- 什么是“loss mask”？
- 为什么只训练回答，不训练用户输入？
- chat template 对模型风格影响有多大？

### 2.3 DPODataset（偏好数据）

- 每条样本含 `chosen` / `rejected`
- 两路样本分别编码
- 生成各自的 loss mask

**必须理解的小点**：
- 为什么 DPO 需要“成对比较”而不是单答案？
- mask 在偏好学习中如何避免无关 token 干扰？

### 2.4 RLAIFDataset（强化学习提示集）

- 把对话构造成 prompt
- 训练时模型自己生成 response，再打 reward

**必须理解的小点**：
- RL 阶段为什么不直接用标准监督标签？
- prompt 与 answer 在 RL 中扮演什么角色？

---

## 3. 训练工具层知识点（trainer_utils）

核心文件：`trainer/trainer_utils.py`

### 3.1 学习率调度

- `get_lr()` 使用余弦风格衰减
- 不是固定 lr，后期会降下来稳定训练

### 3.2 断点续训

- `lm_checkpoint()` 同时保存：
  - 模型参数
  - 优化器状态
  - scaler/scheduler
  - epoch/step
- world size 变化时自动换算 step

### 3.3 初始化与加载

- `init_model()`：构建模型 + 加载 tokenizer + 选择 from_weight
- 注意 `strict=False` 场景（结构兼容加载）

### 3.4 SkipBatchSampler

- 续训时可跳过前面 batch
- 避免每次从头跑

**自媒体选题**：
- 《断点续训为什么是工程能力分水岭？》
- 《我第一次看懂 checkpoint 保存了什么》

---

## 4. 分阶段训练知识点：按脚本拆到“可日学颗粒度”

---

### 4.1 `train_pretrain.py`（预训练）

#### A. 目标
- 学“知识和语言统计规律”（下一 token 预测）。

#### B. 必懂参数
- `max_seq_len`：序列长度上限
- `batch_size` + `accumulation_steps`：等效 batch
- `learning_rate`
- `dtype`
- `from_weight`：是否从已有权重继续

#### C. 核心训练逻辑
- 前向：`model(input_ids, labels=labels)`
- 总损失：`res.loss + res.aux_loss`
- 梯度累积 + 裁剪 + AdamW 更新

#### D. 每日拆分（建议 8 天）
1. Day1：读参数区，理解每个参数作用。
2. Day2：读数据加载与 `PretrainDataset` 对接。
3. Day3：读 `autocast + GradScaler`。
4. Day4：读 loss 构成（logits_loss/aux_loss）。
5. Day5：读梯度累积与 `clip_grad_norm_`。
6. Day6：读保存逻辑（`.pth` + resume）。
7. Day7：跑一次最小配置实验。
8. Day8：改 `max_seq_len` 做一次对照。

---

### 4.2 `train_full_sft.py`（全参数 SFT）

#### A. 目标
- 让模型从“会续写”变成“会按对话指令回答”。

#### B. 与预训练的核心差异
- 数据换成 `SFTDataset`
- 常见学习率更小（防遗忘）
- 重点是输出格式与回答风格

#### C. 每日拆分（建议 6 天）
1. SFT 数据样本结构复盘。
2. chat template 对输出影响。
3. labels 只覆盖 assistant 的原因。
4. 与 pretrain 超参数差异。
5. 小样本跑通并看 loss。
6. 与 pretrain 模型做问答对比。

---

### 4.3 `train_lora.py`（LoRA 微调）

#### A. 目标
- 低显存、低成本地做任务微调。

#### B. 必懂知识点
- 只训练 LoRA 参数，不动全量参数
- `apply_lora` / `save_lora` 的意义
- 为什么 clip 只对 lora params 生效

#### C. 每日拆分（建议 5 天）
1. LoRA 原理（低秩分解）直觉。
2. 看代码里“哪些参数 requires_grad=True”。
3. 比较 full-sft 与 lora 的显存占用。
4. 试不同 rank（若脚本支持扩展）。
5. 导出 LoRA 权重并做推理测试。

---

### 4.4 `train_dpo.py`（偏好优化）

#### A. 目标
- 用 chosen/rejected 对比学习“更受偏好”的回答。

#### B. 核心公式直觉
- 比较策略模型与参考模型在 chosen/rejected 上的对数概率差
- `beta` 控制优化强度

#### C. 必懂实现点
- `logits_to_log_probs`
- `dpo_loss()` 中序列 mask 平均
- 参考模型 `ref_model` 冻结不训练

#### D. 每日拆分（建议 7 天）
1. 读偏好数据格式。
2. 手工算一条 chosen/rejected 的 logprob 差。
3. 理解为什么要 reference model。
4. 理解 `beta` 调大调小后果。
5. 跑最小 DPO。
6. 设计 20 条评测问答。
7. 做 SFT vs DPO 主观打分表。

---

### 4.5 `train_reason.py`（推理蒸馏/思维格式强化）

#### A. 目标
- 让模型稳定输出 `<think>` + `<answer>` 结构。

#### B. 核心机制
- 自定义 token 权重：对 `<think>/<answer>` 标记 token 提高 loss 权重
- 本质是“结构监督增强”

#### C. 每日拆分（建议 5 天）
1. 理解特殊 tag token 的编码。
2. 理解 `loss_mask` 改写逻辑。
3. 观察有/无结构加权对输出格式影响。
4. 记录格式正确率。
5. 产出“推理模板提示词”内容。

---

### 4.6 `train_distillation.py`（白盒蒸馏）

#### A. 目标
- 小学生（student）学习老师（teacher）的分布，不只学硬标签。

#### B. 核心机制
- CE（真值标签） + KL（软标签）混合
- `alpha`：CE 与 KL 权重
- `temperature`：软化分布

#### C. 每日拆分（建议 7 天）
1. 区分硬标签/软标签。
2. 看 `distillation_loss` 的 KL 计算。
3. 理解 `temperature^2` 作用。
4. 读 `alpha*CE + (1-alpha)*KL`。
5. 跑 teacher/student 一组实验。
6. 比较 student 蒸馏前后回答风格。
7. 总结“蒸馏适合什么场景”。

---

### 4.7 `train_ppo.py`（PPO 强化学习）

#### A. 目标
- 在奖励模型约束下，让策略模型持续提升。

#### B. 角色拆分
- Actor：生成回答
- Old Actor：算重要性采样 ratio
- Ref Model：KL 约束参考
- Critic：估计 value
- Reward Model：打分

#### C. 必懂损失
- Policy loss（剪切 PPO）
- Value loss（价值回归）
- KL 惩罚（防策略漂移）
- Aux loss（MoE 时）

#### D. 每日拆分（建议 10 天）
1. 先画 PPO 全流程图。
2. 逐行理解 actor/critic forward。
3. 理解 ratio 与 clip。
4. 理解 advantage 直觉。
5. 理解 KL 与 KL_ref 差异。
6. 理解 update_old_actor_freq。
7. 跑极小 batch PPO。
8. 调 `clip_epsilon`。
9. 调 `vf_coef/kl_coef`。
10. 写 PPO 调参故障手册。

---

### 4.8 `train_grpo.py`（GRPO 强化学习）

#### A. 目标
- 同一 prompt 生成多条答案，做组内相对优势学习。

#### B. 核心机制
- `num_generations` 条回答构成一组
- 组内标准化 reward 得 advantage
- 加 KL 惩罚

#### C. 每日拆分（建议 8 天）
1. 读多样本生成流程。
2. 理解 grouped reward 标准化。
3. 理解 completion_mask 与 eos 截断。
4. 理解 per-token KL。
5. 跑 1 次 reasoning=0。
6. 跑 1 次 reasoning=1。
7. 比较两者格式和回答质量。
8. 总结 GRPO 与 PPO 取舍。

---

### 4.9 `train_spo.py`（SPO 强化学习）

#### A. 目标
- 引入自适应 baseline（价值追踪器）提升稳定性。

#### B. 核心机制
- `AutoAdaptiveValueTracker` 用 Beta 分布思路维护 baseline
- `rho` 根据策略变化自适应调整记忆强度

#### C. 每日拆分（建议 8 天）
1. 读 value tracker 初始化。
2. 理解 alpha/beta 更新。
3. 理解 `rho_mode` 与 KL 相关衰减。
4. 理解 baseline 对 advantage 的作用。
5. 跑一轮并记录 rho 变化。
6. 对比 GRPO 与 SPO 的稳定性。
7. 看长回答下 KL 变化。
8. 总结“何时优先 SPO”。

---

### 4.10 `train_tokenizer.py`（分词器训练，学习用途）

#### A. 目标
- 理解 token 字典如何影响模型表达边界。

#### B. 必懂点
- BPE 训练
- 特殊 token 保留
- chat_template 写入 tokenizer 配置

#### C. 每日拆分（建议 4 天）
1. 看 tokenizers 库训练流程。
2. 理解 vocab_size 与 embedding 参数量关系。
3. 运行 `eval_tokenizer` 看编解码一致性。
4. 结论：生产上通常不重复训练 tokenizer。

---

## 5. 超参数知识图谱（你后续做课程时可直接当一章）

### 5.1 最关键的 12 个超参数

1. `learning_rate`
2. `batch_size`
3. `accumulation_steps`
4. `max_seq_len`
5. `epochs`
6. `grad_clip`
7. `dtype`
8. `beta`（DPO/GRPO/SPO）
9. `clip_epsilon`（PPO）
10. `vf_coef`（PPO）
11. `temperature`（蒸馏）
12. `alpha`（蒸馏）

### 5.2 每个参数你都要回答 4 个问题

- 它控制什么？
- 太大时会怎样？
- 太小时会怎样？
- 在 3050 上推荐起始值是多少？

---

## 6. 常见报错与排障知识点（必须单独学习）

### 6.1 OOM（显存爆）排查顺序

1. 降 `max_seq_len`
2. 降 `batch_size`
3. 开/增大 `accumulation_steps`
4. 用 `float16/bfloat16`
5. 关掉不必要日志与缓存

### 6.2 loss 异常

- 一直不降：学习率可能过小/数据标签有问题
- 震荡很大：学习率偏大、batch 太小
- NaN：混合精度溢出、梯度爆炸、mask 除零

### 6.3 对齐阶段常见问题

- 奖励黑客（投机取巧拿高分）
- 输出格式“看似正确但内容空洞”
- KL 约束过弱导致模型漂移

---

## 7. 你可以直接执行的“90 天日更学习+内容发布计划”

### 7.1 每天固定模板（60~120 分钟）

1. **20 分钟**：读一个最小知识点（只读 30~80 行代码）
2. **20 分钟**：把这段代码画成流程图
3. **20 分钟**：用自己的话写“给小白解释版”
4. **20 分钟**：录/发 1 条内容（小红书/YouTube）
5. **20 分钟（可选）**：做 1 个小实验改 1 个参数

### 7.2 每周交付物（给未来课程做资产沉淀）

- 1 份周报（学到什么/卡点/下周计划）
- 1 个参数对照实验表
- 3~5 条公开内容（图文或视频）
- 1 份“术语白话词典”更新

---

## 8. 内容创业映射：学习内容如何直接变课程素材

### 8.1 从“学习笔记”到“课程章节”

- 你今天学的每个知识点都按“三段式”输出：
  - 概念一句话
  - 代码证据（哪一段）
  - 实验结论（改了什么参数）

### 8.2 三层内容池（避免只做机械规划）

1. **入门池**：术语解释/常见误区
2. **实战池**：训练日志/调参翻车复盘
3. **课程池**：系统化章节（可直接售卖）

### 8.3 你现在就能做的变现前动作

- 建立“公开学习档案”（连续打卡）
- 收集粉丝问题，形成 FAQ 数据库
- 每 2 周做一次免费直播答疑（积累信任）

---

## 9. 最小可执行版本（今天就开始）

如果你只做一件事：

1. 今天读 `train_pretrain.py` 的参数区 + 训练循环。
2. 发一条内容：《我用 8G 显卡第一次看懂大模型训练循环》。
3. 明天读 `SFTDataset`，发第二条内容：《为什么训练时只学 assistant 回答》。

连续 30 天，你就会拥有：

- 一套真实学习轨迹
- 一批可复用课程素材
- 一群因“真实成长”而信任你的第一批用户

---

## 10. 你后续可继续扩展的高级知识点清单（第二阶段）

- MoE 路由与辅助损失
- 长上下文训练（RoPE 外推、长度混训）
- 数据配比策略（Pretrain:SFT:DPO）
- 奖励模型评测与校准
- 拒答、安全、事实性评测框架
- 推理加速（KV cache、量化、推理框架）

> 结论：你不需要“先学完再输出”。最优路径是 **边学边做实验边发布**，把每个训练知识点拆成可完成任务，你的学习效率、内容积累、未来课程规划会同时起飞。

---

## 11. 零基础“任务卡”版本（再细一层，按背景->专业知识->怎么学->怎么练->项目落地）

> 用法：每天只做 1 张任务卡。每张卡控制在 45~90 分钟。  
> 你只需要照着 5 个问题走：
> 1) 这件事的背景是什么？ 2) 涉及什么专业词？ 3) 我先怎么学？ 4) 我怎么练？ 5) 在 MiniMind 里怎么做？

### 11.1 启动周（不懂也能开始）

#### 任务卡 01：你先搞懂“训练”到底在优化什么
- **背景**：很多新手把训练理解成“喂数据就行”，但训练本质是“不断减小损失函数”。
- **专业知识**：`logits`、`loss`、`backward`、`optimizer.step`。
- **怎么学（15 分钟）**：看本文件第 1.1 小节，把 6 步闭环默写一遍。
- **怎么练（15 分钟）**：用一句话解释“为什么 loss 下降说明模型在学习”。
- **项目落地（20 分钟）**：打开 `trainer/train_pretrain.py`，只找 4 行：前向、loss、backward、step。
- **验收标准**：你能不看代码说出训练闭环。

#### 任务卡 02：先懂 token，再谈大模型
- **背景**：模型不认识“字”，只认识 token id。
- **专业知识**：Tokenizer、词表、编码/解码。
- **怎么学（15 分钟）**：阅读 `trainer/train_tokenizer.py` 的 `train_tokenizer` 和 `eval_tokenizer` 函数。
- **怎么练（20 分钟）**：写一段话：为什么同一句中文会被切成多个 token。
- **项目落地（20 分钟）**：在项目中定位 `tokenizer.apply_chat_template` 出现位置，理解模板如何影响输入。
- **验收标准**：你能解释“改词表会影响模型兼容性”。

#### 任务卡 03：理解样本是如何变成训练张量的
- **背景**：训练问题 70% 出在数据处理，不在模型结构。
- **专业知识**：`input_ids`、`labels`、`pad`、`-100(ignore_index)`。
- **怎么学（20 分钟）**：阅读 `dataset/lm_dataset.py` 里的 `PretrainDataset.__getitem__`。
- **怎么练（20 分钟）**：手工画出一条样本从 text 到 input_ids 的流程图。
- **项目落地（20 分钟）**：比较 pretrain 与 sft 的 labels 差异。
- **验收标准**：你能回答“为什么 pad 位置不能参与 loss”。

#### 任务卡 04：SFT 为什么只学 assistant 回答
- **背景**：如果用户 token 也参与训练，模型会“学会复读用户输入”。
- **专业知识**：loss mask、监督信号、chat template。
- **怎么学（20 分钟）**：阅读 `SFTDataset.generate_labels`。
- **怎么练（20 分钟）**：任选一个对话样本，标注哪些 token 应该是 -100。
- **项目落地（20 分钟）**：在文档里写出一条“错误标注会导致什么后果”。
- **验收标准**：你能清楚区分“输入上下文”和“监督目标”。

#### 任务卡 05：理解显存的第一性原理
- **背景**：你后续调参会天天碰到 OOM。
- **专业知识**：batch size、seq len、激活显存、梯度显存。
- **怎么学（15 分钟）**：记住一句：显存压力主要随 batch 和序列长度上涨。
- **怎么练（20 分钟）**：写 3 条“显存不够时优先调整顺序”。
- **项目落地（20 分钟）**：在 `train_pretrain.py` 找 `batch_size`、`max_seq_len`、`accumulation_steps` 参数。
- **验收标准**：你能给出 3050 的“先降谁后降谁”策略。

### 11.2 Pretrain 入门任务卡（从能看懂到能跑通）

#### 任务卡 06：认识 pretrain 脚本的参数分组
- **背景**：新手最怕“参数太多”。
- **专业知识**：训练参数、模型参数、数据参数、日志参数。
- **怎么学（20 分钟）**：把 `train_pretrain.py` 参数分四类抄到笔记。
- **怎么练（20 分钟）**：每类挑 1 个参数，写“调大/调小影响”。
- **项目落地（20 分钟）**：做一张“参数速查表”。
- **验收标准**：你能说出 10 个核心参数用途。

#### 任务卡 07：混合精度到底解决了什么
- **背景**：你看到 `autocast` 和 `GradScaler` 容易懵。
- **专业知识**：float16、bfloat16、数值稳定性。
- **怎么学（20 分钟）**：阅读 pretrain/sft 脚本中的 mixed precision 段落。
- **怎么练（20 分钟）**：写一句通俗解释：为什么半精度更省显存。
- **项目落地（20 分钟）**：记录你设备可用的 dtype 选择。
- **验收标准**：你能说出 `GradScaler` 在 float16 中的作用。

#### 任务卡 08：梯度累积是如何“模拟大 batch”的
- **背景**：小显卡想稳定训练，几乎必须用它。
- **专业知识**：等效 batch、累积步、更新频率。
- **怎么学（20 分钟）**：阅读 `if (step + 1) % args.accumulation_steps == 0` 逻辑。
- **怎么练（20 分钟）**：算一题：batch=1, accumulation=16，等效 batch 是多少？
- **项目落地（20 分钟）**：写出你机器的推荐组合（如 1x16）。
- **验收标准**：你能解释“为什么 loss 要除以 accumulation_steps”。

#### 任务卡 09：学习率调度与训练稳定性
- **背景**：学习率不只是一个常数，它是训练节奏控制器。
- **专业知识**：warm/cool、cosine 衰减、收敛稳定性。
- **怎么学（20 分钟）**：阅读 `trainer_utils.get_lr`。
- **怎么练（20 分钟）**：画一条“先高后低”的学习率曲线。
- **项目落地（20 分钟）**：在日志中定位 `learning_rate` 打印项。
- **验收标准**：你能解释为什么后期降学习率有助于稳定。

#### 任务卡 10：checkpoint 是你的“后悔药”
- **背景**：没有断点续训，训练中断会很崩溃。
- **专业知识**：state_dict、optimizer_state、resume。
- **怎么学（20 分钟）**：阅读 `lm_checkpoint` 保存与加载分支。
- **怎么练（20 分钟）**：写一份“恢复训练步骤清单”。
- **项目落地（20 分钟）**：在本地确认 `save_weight` 命名规则。
- **验收标准**：你知道如何从 step N 继续跑。

### 11.3 SFT 与 LoRA 任务卡（先把“会聊天”做出来）

#### 任务卡 11：pretrain 到 sft 的能力迁移
- **背景**：pretrain 会续写，sft 才会“听指令”。
- **专业知识**：领域迁移、灾难性遗忘。
- **怎么学（20 分钟）**：对比 `train_pretrain.py` 与 `train_full_sft.py` 参数默认值。
- **怎么练（20 分钟）**：写下为什么 sft 学习率更小。
- **项目落地（20 分钟）**：跑一次 `from_weight=pretrain` 的 sft 最小实验。
- **验收标准**：你能解释两阶段目标差异。

#### 任务卡 12：chat template 是“行为说明书”
- **背景**：同一个模型，模板不同，风格完全不同。
- **专业知识**：system/user/assistant 角色、模板注入。
- **怎么学（20 分钟）**：阅读 tokenizer config 中 chat_template（了解即可，不必背）。
- **怎么练（20 分钟）**：写 1 条你理想的 assistant 风格规范。
- **项目落地（20 分钟）**：在 `SFTDataset.create_chat_prompt` 里理解模板入口。
- **验收标准**：你知道“风格问题”常常是模板问题。

#### 任务卡 13：LoRA 为什么便宜
- **背景**：全量微调成本高，LoRA 是创业早期更务实方案。
- **专业知识**：低秩适配、冻结主干、可训练参数占比。
- **怎么学（20 分钟）**：阅读 `train_lora.py` 里 LoRA 参数训练逻辑。
- **怎么练（20 分钟）**：写一句话解释“LoRA 像给大模型加可拆卸插件”。
- **项目落地（20 分钟）**：记录 full-sft vs lora 的显存和速度差。
- **验收标准**：你能解释“为何 LoRA 更适合 8G 显卡”。

### 11.4 偏好优化与推理蒸馏任务卡（做出“更像人”的回答）

#### 任务卡 14：DPO 的直觉先于公式
- **背景**：DPO 并不是让模型“背答案”，而是“偏好比较”。
- **专业知识**：chosen/rejected、reference model、log-ratio。
- **怎么学（20 分钟）**：阅读 `train_dpo.py` 的 `dpo_loss`。
- **怎么练（20 分钟）**：写一个生活类比：同题两答案投票。
- **项目落地（20 分钟）**：准备 20 条固定问题用于前后对比。
- **验收标准**：你能说清 DPO 在优化什么。

#### 任务卡 15：beta 参数怎么理解
- **背景**：beta 太大太小都会有问题。
- **专业知识**：优化强度、保守更新、风格漂移。
- **怎么学（15 分钟）**：阅读 `--beta` 参数注释与日志。
- **怎么练（20 分钟）**：写出 beta=0.05/0.1/0.2 的预期差异。
- **项目落地（20 分钟）**：跑一次小规模对照（至少两组）。
- **验收标准**：你能用实验而非感觉选 beta。

#### 任务卡 16：Reason 训练中的结构监督
- **背景**：推理模型需要稳定输出 `<think>/<answer>`。
- **专业知识**：token 加权、格式奖励、结构化输出。
- **怎么学（20 分钟）**：阅读 `train_reason.py` 中特殊 token 加权段。
- **怎么练（20 分钟）**：标记一条回答里哪些 token 应该被重点学习。
- **项目落地（20 分钟）**：统计 20 条样本的格式正确率。
- **验收标准**：你知道“结构正确≠内容正确”，两者都要测。

#### 任务卡 17：蒸馏的本质是“学分布”
- **背景**：小模型不只学答案，还要学老师的“概率偏好”。
- **专业知识**：KL loss、temperature、soft label。
- **怎么学（20 分钟）**：阅读 `distillation_loss`。
- **怎么练（20 分钟）**：用一句话解释为什么要 `temperature^2`。
- **项目落地（20 分钟）**：做一组 `alpha` 对照实验。
- **验收标准**：你能说明 CE 与 KL 的分工。

### 11.5 强化学习任务卡（PPO/GRPO/SPO，不再只看名词）

#### 任务卡 18：PPO 五角色关系图
- **背景**：PPO难，不是因为公式，而是角色多。
- **专业知识**：actor、critic、old_actor、ref、reward。
- **怎么学（20 分钟）**：阅读 `train_ppo.py` 初始化区域。
- **怎么练（20 分钟）**：手画模块关系图。
- **项目落地（20 分钟）**：给每个模块写一句职责定义。
- **验收标准**：你能不看代码说清 5 个角色。

#### 任务卡 19：优势函数是“相对好坏”
- **背景**：RL 不是“分数越高越好”这么简单。
- **专业知识**：advantage、baseline、ratio clipping。
- **怎么学（20 分钟）**：阅读 PPO/GRPO 中 advantage 相关计算。
- **怎么练（20 分钟）**：用“考试平均分”类比 advantage。
- **项目落地（20 分钟）**：记录一次训练中的 reward 与 loss 同步变化。
- **验收标准**：你能解释为什么要做归一化。

#### 任务卡 20：GRPO 的“组内竞争”
- **背景**：同一题多个回答比较，更利于相对优化。
- **专业知识**：grouped rewards、num_generations、标准化。
- **怎么学（20 分钟）**：阅读 `train_grpo.py` 中 grouped reward 逻辑。
- **怎么练（20 分钟）**：把一组 4 个分数手算标准化结果。
- **项目落地（20 分钟）**：改 `num_generations` 做一次小对照。
- **验收标准**：你知道 GRPO 相比 PPO 在采样上的差异。

#### 任务卡 21：SPO 的自适应 baseline
- **背景**：SPO 重点在“更稳”，不是“更炫”。
- **专业知识**：自适应价值跟踪、rho、beta 分布直觉。
- **怎么学（20 分钟）**：阅读 `AutoAdaptiveValueTracker`。
- **怎么练（20 分钟）**：写 3 条“什么时候需要更稳训练”。
- **项目落地（20 分钟）**：跟踪 rho/baseline 日志变化。
- **验收标准**：你能解释 SPO 为什么更强调稳定性。

### 11.6 每张任务卡的固定输出模板（你拿去发内容就行）

- **今日主题**：
- **一句话背景**：
- **3 个专业词（白话解释）**：
- **我今天看的代码位置**：
- **我做的一个小练习**：
- **我在 MiniMind 里的一个小发现**：
- **明天要验证的一个问题**：

> 你只要连续执行任务卡，就会形成：
> - 可持续学习节奏（不焦虑）
> - 可公开的成长记录（涨信任）
> - 可课程化的知识资产（未来变现）
